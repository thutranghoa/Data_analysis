{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def scalp_page():\n",
    "    table1_body  = driver.find_element(By.CLASS_NAME, 'dgrid-content') \n",
    "    table1_row = table1_body.find_elements (By.CLASS_NAME, 'dgrid-row')\n",
    "\n",
    "    # find get all row\n",
    "    f1 =  open ('table1_.csv', 'a') \n",
    "    writer1 = csv.writer(f1)\n",
    "\n",
    "\n",
    "\n",
    "    row2 = []\n",
    "    row1 = []\n",
    "    row3 = []\n",
    "    for r in table1_row:\n",
    "        r.click()\n",
    "        driver.implicitly_wait(30)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        'table1'\n",
    "\n",
    "        r_html = r.get_attribute('outerHTML')\n",
    "        dff=pd.read_html(r_html)\n",
    "        print(dff)\n",
    "\n",
    "        table_data1 = r.find_elements (By.TAG_NAME, 'td')\n",
    "        # st = table_data.text\n",
    "        print ('-----------------------------')\n",
    "        row = []\n",
    "        for data in table_data1 :\n",
    "            row.append(data.text)\n",
    "        print (row)\n",
    "        row1.append(row)\n",
    "\n",
    "        'table2'\n",
    "        # time.sleep(0.5) # IMPORTANT VAI \n",
    "        # table2 = driver.find_element(By.XPATH, '/html/body/div[3]/div[3]/div[1]/div/div/div[2]/div[3]/div[2]/div[2]/div/div/table')\n",
    "        # table2_html = table2.get_attribute('outerHTML')\n",
    "        # df1=pd.read_html(table2_html)\n",
    "        # df1 = pd.concat(df1)\n",
    "        # # row2.append(df)\n",
    "        # df1.to_csv('table2_.csv', mode = 'a',header = False)\n",
    "        # # print(df)\n",
    "        # # print (row2)\n",
    "\n",
    "        # 'table3'\n",
    "        # time.sleep(0.5)\n",
    "        # table3 = driver.find_element(By.XPATH, '//*[@id=\"dsdata_pane\"]/table[2]')\n",
    "\n",
    "        # table3_html = table3.get_attribute('outerHTML')\n",
    "        # df2 = pd.read_html(table3_html)\n",
    "        # df2 = pd.concat(df2)\n",
    "        # df2.to_csv('table3_.csv', mode = 'a',header = False)\n",
    "\n",
    "        # row3.append(df2)\n",
    "\n",
    "    writer1.writerows(row1)\n",
    "    \n",
    "    'Read table Components'\n",
    "\n",
    "\n",
    "url = 'https://ilthermo.boulder.nist.gov/'\n",
    "# keyword_search = \"carbon\"\n",
    "property_name = \"Eutectic composition\"\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "searchButton = driver.find_element(By.ID, \"sbutton_label\")\n",
    "searchButton.click()\n",
    "\n",
    "# Find the search field and enter a string\n",
    "search_field = driver.find_element(By.ID, \"cmp\")\n",
    "# search_field.send_keys(keyword_search)\n",
    "\n",
    "# Find property box to click\n",
    "dropdowns = driver.find_elements(By.TAG_NAME, \"table\") \n",
    "time.sleep(1)\n",
    "WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, '/html/body/div[4]/div[2]/div[1]/table[2]/tbody/tr/td[1]/div[1]'))).click() # cha hieu lam, le ra chi can cai nay la du, nhung phai co 2 dong tren thi cai nay moi hoat dong\n",
    "\n",
    "prp_dropdown = driver.find_elements(By.CSS_SELECTOR, 'tr')\n",
    "property_found = False\n",
    "property_name_lower = property_name.lower().strip()\n",
    "for item in prp_dropdown:\n",
    "    itemName = item.text.lower().strip()\n",
    "    if (itemName == property_name_lower):\n",
    "        property_found = True\n",
    "        item.click()\n",
    "\n",
    "if (not property_found):\n",
    "    print(\"PROPERTY NOT FOUND !!!\")\n",
    "    exit(0)\n",
    "\n",
    "# Press Enter to submit the search\n",
    "search_field.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the page to load (you may need to adjust the time depending on your application)\n",
    "driver.implicitly_wait(15)\n",
    "\n",
    "\n",
    "scalp_page() # IDK, BUT HAVE TO SCALP THE FIRST PAGE SO NUMPAGE CAN BE EXTRACTED\n",
    "\n",
    "# get num pages\n",
    "pages = driver.find_elements(By.CLASS_NAME, 'dgrid-page-link')\n",
    "\n",
    "\n",
    "'''\n",
    "id- component : dscomp\n",
    "id- data : dsdata\n",
    "\n",
    "'''\n",
    "numPages_str = pages[-2].text\n",
    "numPages = 1\n",
    "if (numPages_str != '1' and numPages_str != '<' and numPages_str != '>'):\n",
    "    numPages = int(numPages_str)\n",
    "print(\"There are\", numPages, \"pages\")\n",
    "\n",
    "for i in range(numPages - 1):\n",
    "    # click > button to go to the next page\n",
    "    button = driver.find_element(By.XPATH, '/html/body/div[3]/div[3]/div[1]/div/div/div[2]/div[1]/div/div[4]/div/div[2]/span[3]')\n",
    "    button.click()\n",
    "    driver.implicitly_wait(20)\n",
    "    scalp_page()\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# page = requests.get(url)\n",
    "\n",
    "# if page.status_code == 200:\n",
    "#     soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#     print(soup)\n",
    "\n",
    "#     # Find all tables on the page\n",
    "#     tables = soup.find_all(class_='dgrid-row-table')\n",
    "\n",
    "#     # Process and print data from each table\n",
    "#     for i, table in enumerate(tables, 1):\n",
    "#         for row in table.find_all('tr'):\n",
    "#             # Extract and print data from each row/column\n",
    "#             columns = row.find_all(['th', 'td'])\n",
    "#             row_data = [col.text.strip() for col in columns]\n",
    "#             print(row_data)\n",
    "\n",
    "#         # print(\"\\n\" + \"=\"*30 + \"\\n\")  # Separating tables with a line\n",
    "\n",
    "# else:\n",
    "#     print(f\"Failed to fetch the page. Status code: {page.status_code}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# linh tinh\n",
    "# driver.implicitly_wait(50)\n",
    "\n",
    "# prp = driver.find_elements(By.XPATH, \"/html/body/div[4]/div[2]/div[1]/table[2]/tbody/tr/td[1]/div[1]\")\n",
    "# prp = driver.find_elements(By.CSS_SELECTOR, \"#prp\") #prp\n",
    "# prp[0].click()\n",
    "# prp_dropdown = driver.find_elements(By.XPATH, '//*[@id=\"prp_menu\"]/tbody')\n",
    "# dropdown = Select(driver.find_elements(By.XPATH, '//*[@id=\"prp_dropdown\"]'))\n",
    "# print(len(dropdown))\n",
    "\n",
    "\n",
    "# if (prp[0].text == '-- unspecified --'):\n",
    "#     print(\"OH YEAH\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
